---
title: "Correlation and Regression 2"
author: "Chris Qi"
date: "10/13/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# tutorial::go_interactive()
```

![](/Users/yuandong/Dropbox/Public/logo.png){width=500px}

课程回放1:
(https://www.cctalk.com/v/15406449969744?xh_fshareuid=87b7f11f-6ceb-c339-6ae2-1632fe5f61ea&xh_preshareid=f0b6bd9c-0d1e-4cca-9c81-56e1d163e617)

课程回放2:
(https://www.cctalk.com/v/15406498208704?xh_fshareuid=87b7f11f-6ceb-c339-6ae2-1632fe5f61ea&xh_preshareid=e5305ddb-7fff-42d3-ba04-b984d1830612)

![](https://www.dropbox.com/s/a5okpste5xhbglo/regression_joke.png?dl=1){width=600px}



```{r echo=FALSE}
#library(ggplot2)

plot(mtcars$wt, mtcars$mpg,
     xlab="Miles Per Gallon",
     ylab="Car Weight")
abline(lm(mtcars$mpg~mtcars$wt))
title("Regression of MPG on Weight")
```

$$y_i= a + b*x_i+e_i$$

### 回归模型的目标是量化变量之间的关系：

* 关键系数的大小

* 关键系数的显著度

* 模型的解释力

* 模型的预测力

```{r echo=FALSE}
out <- lm(mpg~wt,data=mtcars)
summary(out)
```


理解模型：

1. 残差 （Residuals）：
Residuals show if the predicted response values are close or not to the response values that the model predicts. 

$$e_i = y_i-\hat{y_i}$$

2. 系数估计 （Estimate coefficient）：
How y changes when x changes by one unit

$$\hat{b}=\frac{\sum (x_i-\bar{x}) (y_i-\bar{y})}{\sum (x_i-\bar{x})^2}$$

$$\hat{a}= \bar{y} - \hat{b}\bar{x}$$

3. 系数估计的标准差（Standard error）：
Standard error measures how the coefficient estimates can vary from the actual average value of the response variable (i.e. if the model is run more times). 每一次数据不同，结果都会有差异，但是该差异在一定范围内。 

4. 显著度与P值 （Significance and P value）：
Test of significance of the model shows that there is strong evidence of a linear relationship between the variables. This is visually interpreted by the significance stars *** at the end of the row. This level is a threshold that is used to indicate real findings, and not the ones by chance alone.

For each estimated regression coefficient, the variable’s p-Value Pr(>|t|) provides an estimate of the probability that the true coefficient is zero given the value of the estimate. More the number of stars near the p-Value are, more significant is the variable.

With the presence of the p-value, there is a test of hypothesis associated with it. In Linear Regression, the Null Hypothesis is that the coefficient associated with the variables is equal to zero. Instead, the alternative hypothesis is that the coefficient is not equal to zero and then exists a relationship between the independent variable and the dependent variable.

So, if p-values are less than significance level (typically, a p-value < 0.05 is a good cut-off point), null hypothesis can be safely rejected. In the current case, p-values are well below the 0.05 threshold, so the model is indeed statistically significant.

5. R-squared 以及 adjusted R-squared
For the simple linear regression, R-squared is the square of the correlation between two variables. Its value can vary between 0 and 1: a value close to 0 means that the regression model does not explain the variance in the response variable, while a number close to 1 that the observed variance in the response variable is well explained. In the current case, R-squared suggests the linear model fit explains about 75% of the variance observed in the data.

$$R^2=1-\frac{var(e)}{var(y)}$$
$$R^2_{adj}=1-\frac{var(e)/(n-q)}{var(y)/(n-1)}=1-\frac{(1-R^2)(n-1)}{n-q}$$


6.F statistic
Basically, F-test compares the model with zero predictor variables (the intercept only mod1el), and suggests whether the added coefficients improve the model. If a significant result is obtained, then the coefficients (in the current case, being a simple regression, only one predictor is entered) included in the model improve the model’s fit.
So, F statistic defines the collective effect of all predictor variables on the response variable. In this model, F=119.8 is far greater than 1.

模型评估标准汇总：
![](https://www.dropbox.com/s/6potui9c9syfq35/modelassess.png?dl=1)

### 相关系数分析
记得在自己本地电脑上安装`install.packages("corrplot")`

查看数据前几名：
```{r}
head(mtcars)
```

所有变量间的相关系数：
```{r}
cor(mtcars)
```

保存所有相关系数为`M`
```{r}
library(corrplot)
M<-cor(mtcars)
head(round(M,2))
```

corrplot主要有数种展现相关性的方法：“circle”,"pie", “color”, “number”。
一般表达式为：
```{r eval=FALSE}
corrplot(corr, method="circle")
```

```{r}
library(corrplot)
M<-cor(mtcars)
corrplot(M, method="circle")

```

```{r}
library(corrplot)
M<-cor(mtcars)
corrplot(M, method="pie")

```

```{r}
library(corrplot)
M<-cor(mtcars)
corrplot(M, method="color")

```

```{r}
library(corrplot)
M<-cor(mtcars)
corrplot(M, method="number")

```

```{r}
library(corrplot)
M<-cor(mtcars)
corrplot(M, type="upper")

```

```{r}
library(corrplot)
M<-cor(mtcars)
corrplot(M, type="lower")

```


```{r}
library(corrplot)
library(RColorBrewer)

M<-cor(mtcars)
corrplot(M, type="upper", 
         col=brewer.pal(n=8, name="RdBu"))

```

```{r}
library(corrplot)
library(RColorBrewer)

corrplot(M, type="upper",
         col=brewer.pal(n=8, name="RdYlBu"))

```

```{r}
library(corrplot)
library(RColorBrewer)

corrplot(M, type="upper",
         col=brewer.pal(n=8, name="PuOr"))

```

改字体颜色以及字体方向：
```{r}
library(corrplot)
library(RColorBrewer)

corrplot(M, type="upper", tl.col="black", tl.srt=45)

```

改变背景颜色：
```{r}
library(corrplot)
M<-cor(mtcars)
# Change background color to lightblue
corrplot(M, type="upper", col=c("black", "white"),
         bg="lightblue")

```

使用下面的公式，计算相关性的P值，是否显著相关：
```{r}
# mat : is a matrix of data
# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(mtcars)
head(p.mat[, 1:5])
```

```{r}
library(corrplot)
M<-cor(mtcars)

# Specialized the insignificant value according to the significant level
corrplot(M, type="upper",
         p.mat = p.mat, sig.level = 0.01)

```

```{r}
library(corrplot)
M<-cor(mtcars)

# Leave blank on no significant coefficient
corrplot(M, type="upper",
         p.mat = p.mat, sig.level = 0.01, insig = "blank")
```

```{r}
library(corrplot)
M<-cor(mtcars)

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(M, method="color", col=col(200),  
         type="upper",
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         # Combine with significance
         p.mat = p.mat, sig.level = 0.01, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
         )
```

使用 `corrplot()`函数 作出炫酷的相关性矩阵 correlation matrix，你学会了吗？
![](/Users/yuandong/Dropbox/Public/Uncle_Sam.png){width=200px}

再教你一招，来补一刀！
`install.packages("PerformanceAnalytics")`

```{r}
library("PerformanceAnalytics")
my_data <- mtcars[, c(1,3,4,5,6,7)]
chart.Correlation(my_data, histogram=TRUE, pch=19)
```


### 回归分析

用箱图检查异常值：

Generally, any datapoint that lies outside the 1.5 X interquartile-range (1.5 X IQR) is considered an outlier, where, IQR is calculated as the distance between the 25th percentile and 75th percentile values for that variable.
```{r}
boxplot(mtcars)
```

```{r}

par(mfrow=c(1, 2))  # divide graph area in 2 columns
boxplot(mtcars$wt, main="Weigt") 
boxplot(mtcars$hp, main="horse power") 
```

线性回归，一起来理解一下下面两个模型的结果：

```{r}
library(ggplot2)
ggplot(data = mtcars, aes(x = wt, y = hp)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
model_1 <- lm(hp~wt,data=mtcars)
summary(model_1)
```

```{r}
fitted.values(model_1)
```

```{r}
residuals(model_1)
```

```{r}
# Scatterplot with regression line
ggplot(data = airquality, aes(x = Temp, y = Wind)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
model_2 <- lm(Wind~Temp,data=airquality)
summary(model_2)
```

```{r}
fitted.values(model_2)
```

```{r}
residuals(model_2)
```

我们可以将最终得到的模型用于新的数据来做预测，这是机器学习的基础。

